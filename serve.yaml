envs:
  MODEL_NAME: $MODEL_NAME

resources:
  accelerators: A10:1

setup: |
  conda activate vllm
  if [ $? -ne 0 ]; then
    conda create -n vllm python=3.10 -y
    conda activate vllm
  fi

  git clone https://github.com/vllm-project/vllm.git || true
  # Install fschat and accelerate for chat completion
  pip install fschat
  pip install accelerate

  cd vllm
  pip list | grep vllm || pip install .
  pip install gradio


run: |
  conda activate vllm
  echo 'Starting vllm api server...'
  python -u -m vllm.entrypoints.api_server \
                   --model $MODEL_NAME \
                   --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
                   --tokenizer $MODEL_NAME 2>&1 | tee api_server.log &

  echo 'Waiting for vllm api server to start...'
  while ! `cat api_server.log | grep -q 'Uvicorn running on'`; do sleep 1; done

  echo 'Starting gradio server...'
  python vllm/examples/gradio_webserver.py